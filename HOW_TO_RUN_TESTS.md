# How to Run Generated Test Cases

This guide explains how to execute the test cases generated by the QA_AI test case generators.

## üìã Prerequisites

Before running the test cases, ensure you have the required dependencies:

```bash
# Install required packages
pip install pytest playwright pandas rich openai asyncio

# Install Playwright browsers
playwright install chromium
```

## üöÄ Running Test Cases

### Method 1: Run Individual Test Scripts

Each generated test script can be run independently using pytest:

```bash
# Navigate to the test directory
cd test_cases/shop.deere.com_us_diagrams_dealer-id_036816_story_st969494_catalog_no_11945/test_scripts

# Run a specific test file
pytest test_form_1_search_form_20250709_164621.py -v

# Run all test files in the directory
pytest *.py -v

# Run with browser visible (non-headless)
pytest test_form_1_search_form_20250709_164621.py -v --headed
```

### Method 2: Run All Tests for a Domain

```bash
# From the project root, run all tests for a specific domain
pytest test_cases/shop.deere.com_us_diagrams_dealer-id_036816_story_st969494_catalog_no_11945/test_scripts/ -v

# Run with parallel execution (faster)
pytest test_cases/shop.deere.com_us_diagrams_dealer-id_036816_story_st969494_catalog_no_11945/test_scripts/ -v -n auto
```

### Method 3: Run Tests with Custom Configuration

```bash
# Run with custom timeout
pytest test_form_1_search_form_20250709_164621.py -v --timeout=30

# Run specific test methods
pytest test_form_1_search_form_20250709_164621.py::TestForm1::test_tc001_positive_high -v

# Run tests matching a pattern
pytest test_form_1_search_form_20250709_164621.py -k "positive" -v
```

## üîß Understanding the Generated Test Structure

### Test Script Structure

Each generated test script contains:

```python
class TestForm1:
    """Test cases for Search Form form"""
    
    @pytest.fixture(scope="class")
    def browser(self):
        # Browser setup
        pass
    
    @pytest.fixture(scope="class")
    def page(self, browser):
        # Page setup
        pass

    def test_tc001_positive_high(self, page):
        """Positive test case"""
        # Test implementation
        pass

    def test_tc002_negative_medium(self, page):
        """Negative test case"""
        # Test implementation
        pass
```

### Test Categories

The generated tests include:

1. **Positive Tests** (`test_tc001_positive_high`): Valid input scenarios
2. **Negative Tests** (`test_tc002_negative_medium`): Invalid input scenarios
3. **Edge Cases** (`test_tc003_edge_case_low`): Boundary conditions
4. **Accessibility Tests** (`test_tc004_accessibility_high`): Screen reader support
5. **Dynamic Tests** (`test_tc005_dynamic_medium`): Dynamic behavior testing

## üìä Test Output Formats

### 1. JSON Test Cases
```bash
# View JSON test cases
cat test_cases/shop.deere.com_us_diagrams_dealer-id_036816_story_st969494_catalog_no_11945/json/form_1_search_form_*.json
```

### 2. CSV Test Cases
```bash
# View CSV test cases
cat test_cases/shop.deere.com_us_diagrams_dealer-id_036816_story_st969494_catalog_no_11945/csv/form_1_search_form_*.csv
```

### 3. Markdown Reports
```bash
# View detailed reports
cat test_cases/shop.deere.com_us_diagrams_dealer-id_036816_story_st969494_catalog_no_11945/reports/form_1_search_form_*.md
```

## üõ†Ô∏è Customizing Test Execution

### 1. Modify Test Scripts

The generated test scripts have TODO comments that need to be implemented:

```python
def test_tc001_positive_high(self, page):
    """
    Valid Input - Search by Part Number
    Type: positive
    Priority: high
    """
    # TODO: Implement test steps
    # Navigate to the form URL
    # Enter a valid part number into the search input field
    # Press the Enter key
    
    # Expected: Search results related to the part number should be displayed
    pass
```

### 2. Add Test Implementation

Replace the TODO comments with actual test steps:

```python
def test_tc001_positive_high(self, page):
    """Valid Input - Search by Part Number"""
    
    # Navigate to the form
    page.goto("https://shop.deere.com/us/diagrams?dealer-id=036816&story=st969494&catalog_no=11945")
    
    # Wait for search input to be available
    search_input = page.wait_for_selector('input[type="search"]')
    
    # Enter valid part number
    search_input.fill("RE12345")
    
    # Press Enter
    search_input.press("Enter")
    
    # Wait for results
    page.wait_for_selector('.search-results', timeout=10000)
    
    # Assert results are displayed
    results = page.query_selector('.search-results')
    assert results is not None
```

### 3. Add Assertions

```python
# Check for specific elements
assert page.query_selector('.search-results') is not None

# Check for text content
assert "Part found" in page.content()

# Check for form validation
assert page.query_selector('.error-message') is None
```

## üîÑ Continuous Integration

### 1. GitHub Actions Example

```yaml
name: Run Generated Tests
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install pytest playwright pandas rich openai asyncio
        playwright install chromium
    
    - name: Run tests
      run: |
        pytest test_cases/*/test_scripts/ -v --headed
```

### 2. Local CI Script

```bash
#!/bin/bash
# run_tests.sh

echo "Running all generated tests..."

# Run tests for all domains
for domain_dir in test_cases/*/; do
    if [ -d "$domain_dir/test_scripts" ]; then
        echo "Running tests for $(basename "$domain_dir")"
        pytest "$domain_dir/test_scripts/" -v
    fi
done

echo "All tests completed!"
```

## üêõ Troubleshooting

### Common Issues

1. **Browser Launch Failures**
```bash
# Reinstall Playwright
playwright install chromium

# Run with system browser
pytest test_form_1_search_form_20250709_164621.py -v --headed
```

2. **Timeout Issues**
```bash
# Increase timeout
pytest test_form_1_search_form_20250709_164621.py -v --timeout=60
```

3. **Element Not Found**
```python
# Add explicit waits
page.wait_for_selector('input[type="search"]', timeout=10000)
```

4. **Network Issues**
```python
# Add network wait
page.wait_for_load_state('networkidle')
```

### Debug Mode

```bash
# Run with debug output
pytest test_form_1_search_form_20250709_164621.py -v -s

# Run with browser visible
pytest test_form_1_search_form_20250709_164621.py -v --headed

# Run with slow motion
pytest test_form_1_search_form_20250709_164621.py -v --headed --slowmo 1000
```

## üìà Test Reporting

### Generate HTML Reports

```bash
# Install pytest-html
pip install pytest-html

# Generate HTML report
pytest test_cases/*/test_scripts/ -v --html=test_report.html --self-contained-html
```

### Generate JUnit XML Reports

```bash
# Generate XML report for CI systems
pytest test_cases/*/test_scripts/ -v --junitxml=test_results.xml
```

## üéØ Best Practices

1. **Start Small**: Run individual test files first
2. **Implement Gradually**: Add test implementations one by one
3. **Use Headed Mode**: See what's happening during development
4. **Add Logging**: Include detailed logging for debugging
5. **Handle Flaky Tests**: Add retry logic for dynamic content
6. **Parallel Execution**: Use `-n auto` for faster execution
7. **Regular Updates**: Re-generate tests when forms change

## üìö Next Steps

1. **Implement Test Logic**: Replace TODO comments with actual test steps
2. **Add Custom Assertions**: Include domain-specific validations
3. **Integrate with CI/CD**: Set up automated test execution
4. **Customize for Your Framework**: Adapt to your preferred testing framework
5. **Add Test Data**: Include realistic test data for your application

---

**Happy Testing! üß™‚ú®** 